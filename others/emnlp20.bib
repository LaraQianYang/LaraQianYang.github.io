@inproceedings{li-etal-2020-improving,
    title = "Improving Text Generation with Student-Forcing Optimal Transport",
    author = "Li, Jianqiao  and
      Li, Chunyuan  and
      Wang, Guoyin  and
      Fu, Hao  and
      Lin, Yuhchen  and
      Chen, Liqun  and
      Zhang, Yizhe  and
      Tao, Chenyang  and
      Zhang, Ruiyi  and
      Wang, Wenlin  and
      Shen, Dinghan  and
      Yang, Qian  and
      Carin, Lawrence",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.735",
    doi = "10.18653/v1/2020.emnlp-main.735",
    pages = "9144--9156",
    abstract = "Neural language models are often trained with maximum likelihood estimation (MLE), where the next word is generated conditioned on the ground-truth word tokens. During testing, however, the model is instead conditioned on previously generated tokens, resulting in what is termed exposure bias. To reduce this gap between training and testing, we propose using optimal transport (OT) to match the sequences generated in these two modes. We examine the necessity of adding Student-Forcing scheme during training with an imitation learning interpretation. An extension is further proposed to improve the OT learning for long sequences, based on the structural and contextual information of the text sequences. The effectiveness of the proposed method is validated on machine translation, text summarization, and text generation tasks.",
}